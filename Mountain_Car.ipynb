{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Pile_layer():\n",
    "    def __init__(self, dimension, input_dim, amount, offset):\n",
    "        self.dimension = dimension\n",
    "        self.amount = amount\n",
    "        self.offset = offset\n",
    "        self.input_dim = input_dim\n",
    "        self.initialize_layer()\n",
    "        self.total_dimension = (dimension[0]+1)*(dimension[1]+1)*amount\n",
    "    \n",
    "    def initialize_layer(self):\n",
    "        width = []\n",
    "        self.x_ranges = []\n",
    "        self.y_ranges = []\n",
    "        for i in range(len(self.dimension)):\n",
    "            width.append((input_dim[i][1]-input_dim[i][0])/(self.dimension[i]))\n",
    "        for num in range(self.amount):\n",
    "            this_x_offset = self.offset[0]*width[0]*num\n",
    "            this_y_offset = self.offset[1]*width[1]*num\n",
    "            this_x_ranges = []\n",
    "            this_y_ranges = []\n",
    "            for i in range(self.dimension[0]+1):\n",
    "                if i == 0:\n",
    "                    this_x_ranges.append(self.input_dim[0][0])\n",
    "                    if this_x_offset != 0:\n",
    "                        this_x_ranges.append(self.input_dim[0][0]+this_x_offset+width[0]*i)\n",
    "                elif i == self.dimension[0]:\n",
    "                    this_x_ranges.append(self.input_dim[0][1])\n",
    "                    if this_x_offset == 0:\n",
    "                        this_x_ranges.append(-np.inf)\n",
    "                else:\n",
    "                    this_x_ranges.append(self.input_dim[0][0]+this_x_offset+width[0]*i)                        \n",
    "                        \n",
    "            for i in range(self.dimension[1]+1):\n",
    "                if i == 0:\n",
    "                    this_y_ranges.append(self.input_dim[1][0])\n",
    "                    if this_y_offset != 0:\n",
    "                        this_y_ranges.append(self.input_dim[1][0]+this_y_offset+width[1]*i)\n",
    "                elif i == self.dimension[1]:\n",
    "                    this_y_ranges.append(self.input_dim[1][1])\n",
    "                    if this_y_offset == 0:\n",
    "                        this_y_ranges.append(-np.inf)                    \n",
    "                else:\n",
    "                    this_y_ranges.append(self.input_dim[1][0]+this_y_offset+width[1]*i) \n",
    "        \n",
    "            self.x_ranges.append(this_x_ranges)\n",
    "            self.y_ranges.append(this_y_ranges)\n",
    "            \n",
    "        self.x_ranges = np.asarray(self.x_ranges)\n",
    "        self.y_ranges = np.asarray(self.y_ranges)\n",
    "    \n",
    "    def get_vector(self, state):\n",
    "        state_matrix = np.zeros((self.amount, self.dimension[0]+1, self.dimension[1]+1))\n",
    "        for num in range(self.amount):\n",
    "            x_tile = np.argmin([np.inf if x<=0 else x for x in self.x_ranges[num]-state[0]])-1\n",
    "            y_tile = np.argmin([np.inf if x<=0 else x for x in self.y_ranges[num]-state[1]])-1\n",
    "            state_matrix[num, x_tile, y_tile] = 1\n",
    "        \n",
    "        return state_matrix.flatten()\n",
    "            \n",
    "            \n",
    "\n",
    "input_dim = [[env.observation_space.low[0],env.observation_space.high[0]],[env.observation_space.low[1],env.observation_space.high[1]]]\n",
    "piler = Pile_layer([15,15],input_dim, 5, [0.2,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5087216,  0.       ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "#piler.get_vector([0,0]).reshape((5,9,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "\n",
    "actions_shared = [0,1,2]\n",
    "\n",
    "# Define shared parameters\n",
    "gamma_shared = 0.9\n",
    "epsilon_shared = 0.15\n",
    "num_episodes_shared = 2\n",
    "num_agents_shared = 1\n",
    "alpha_shared = 0.2\n",
    "\n",
    "#actions_shared = env.get_actions()\n",
    "num_actions_shared = len(actions_shared)\n",
    "\n",
    "pi_init_shared = [1/num_actions_shared for _ in range(num_actions_shared)] #uniform distribution\n",
    "q_init_shared = 0 # Initialize state-action-pairs that have not been seen yet with 0\n",
    "\n",
    "def init_pi_q(pi, q, pi_init, q_init, state, actions, terminal):\n",
    "    # function to initialize pi and q when states/state-action-pairs are visited the first time\n",
    "    # In contrast to initializing the variables for each state/state-action-pair at the beginning\n",
    "    # this reduces the size of pi and q and thus reduces memory requirements and reduces computation time by approx. 50%\n",
    "    if state not in pi.keys():\n",
    "        pi[state] = pi_init\n",
    "        for action in actions:\n",
    "            if terminal:\n",
    "                q[(state, action)] = 0\n",
    "            else:\n",
    "                q[(state, action)] = q_init\n",
    "    \n",
    "    return pi, q\n",
    "\n",
    "def init_pi_q_e(pi, q, e, pi_init, q_init, state, actions, terminal):\n",
    "    # function to initialize pi and q when states/state-action-pairs are visited the first time\n",
    "    # In contrast to initializing the variables for each state/state-action-pair at the beginning\n",
    "    # this reduces the size of pi and q and thus reduces memory requirements and reduces computation time by approx. 50%\n",
    "    if state not in pi.keys():\n",
    "        pi[state] = pi_init\n",
    "        for action in actions:\n",
    "            e[(state, action)] = 0            \n",
    "            if terminal:\n",
    "                q[(state, action)] = 0\n",
    "            else:\n",
    "                q[(state, action)] = q_init\n",
    "    \n",
    "    return pi, q, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "epsilon_start = 0.15\n",
    "num_episodes = 1000\n",
    "num_agents = 1\n",
    "pi_init = pi_init_shared\n",
    "q_init = q_init_shared\n",
    "\n",
    "actions = actions_shared\n",
    "num_actions = len(actions)\n",
    "\n",
    "sarsa_rewards = []\n",
    "agent_id = 0\n",
    "\n",
    "pi_init = [1/num_actions for _ in range(num_actions)]\n",
    "q_init = 0\n",
    "\n",
    "while agent_id < num_agents:\n",
    "    agent_id += 1\n",
    "\n",
    "    # Initialize policy \"pi\", state-action-pair-values \"q\" and dict of returns\n",
    "    # Dictionaries will be filled during training, Epsilon-soft pi guarantees that each state will be visited (for infinite episodes)\n",
    "    pi = {}\n",
    "    sap = {}\n",
    "    q = {}\n",
    "    returns = {}\n",
    "    theta = np.full((3, piler.total_dimension), 1, dtype=float)\n",
    "\n",
    "    episode_id = 0\n",
    "    this_agent_rewards = []\n",
    "    #this_agent_q = np.zeros((3, piler.total_dimension))\n",
    "    #counter = 0\n",
    "    finished = False\n",
    "    while episode_id < num_episodes:\n",
    "        episode_id += 1\n",
    "        epsilon = epsilon_start-epsilon_start/(num_episodes)*episode_id\n",
    "        #if episode_id%10==0:\n",
    "        #    print(episode_id)\n",
    "        this_episode_sap = []\n",
    "        this_episode_rewards = []\n",
    "        this_state = env.reset()\n",
    "        #print(this_state)\n",
    "        this_state = piler.get_vector(this_state)\n",
    "        #this_state_hash = hash(str(this_state))\n",
    "        #print(this_state_hash)\n",
    "        terminal = False\n",
    "        #pi, q = init_pi_q(pi, q, pi_init, q_init, this_state_hash, actions, terminal)\n",
    "        q_all_a = np.array([np.sum(theta[x]*this_state) for x in actions])\n",
    "        a_star = np.random.choice(np.flatnonzero(q_all_a == q_all_a.max()))\n",
    "        #pi[this_state_hash] = [epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions]\n",
    "        this_action = np.random.choice(actions, p=[epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions])\n",
    "        #print(theta[this_action].shape,this_state.shape)\n",
    "        this_q = np.sum(theta[this_action]*this_state)\n",
    "           \n",
    "        #print(q_all_a)\n",
    "        #print(pi[this_state_hash])\n",
    "        #(next_state, reward, terminal, _) = env.step(this_action)\n",
    "        #print(next_state)\n",
    "        #print(reward)\n",
    "        #print(terminal)\n",
    "        counter=0\n",
    "        while True:\n",
    "            counter += 1\n",
    "            (next_state, reward, terminal, _) = env.step(this_action)\n",
    "            #if terminal and counter<200:\n",
    "            #    print(episode_id, 'Finished')\n",
    "            #print(next_state[0])\n",
    "            #if next_state[0]<-1.2: \n",
    "            #    this_state = env.reset()\n",
    "            #    #print(this_state)\n",
    "            #    this_state = piler.get_vector(this_state)\n",
    "            #    this_state_hash = hash(str(this_state))\n",
    "            #    #print(this_state_hash)\n",
    "            #    terminal = False\n",
    "            #    #pi, q = init_pi_q(pi, q, pi_init, q_init, this_state_hash, actions, terminal)\n",
    "            #    q_all_a = np.array([np.sum(theta[x]*this_state) for x in actions])\n",
    "            #    a_star = np.random.choice(np.flatnonzero(q_all_a == q_all_a.max()))\n",
    "            #    #pi[this_state_hash] = [epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions]\n",
    "            #    this_action = np.random.choice(actions, p=[epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions])\n",
    "            #    #print(theta[this_action].shape,this_state.shape)\n",
    "            #    this_q = np.sum(theta[this_action]*this_state)\n",
    "            #if next_state[0]>0.6:\n",
    "            #    reward = 10\n",
    "            #    terminal = True\n",
    "            #print(next_state)\n",
    "            next_state = piler.get_vector(next_state)\n",
    "            #next_state_hash = hash(str(next_state))\n",
    "            #pi, q = init_pi_q(pi, q, pi_init, q_init, next_state_hash, actions, terminal)\n",
    "            q_all_a = np.array([np.sum(theta[x]*next_state) for x in actions])\n",
    "            a_star = np.random.choice(np.flatnonzero(q_all_a == q_all_a.max()))\n",
    "            #pi[next_state_hash] = [epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions]\n",
    "            next_action = np.random.choice(actions, p=[epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions])\n",
    "            next_q = np.sum(theta[next_action]*next_state)\n",
    "            #print(next_action)\n",
    "            #print(np.sum(theta[this_action]*this_state))\n",
    "            theta[this_action] = theta[this_action] +alpha * (reward+gamma*next_q-this_q)*this_state\n",
    "            #print(np.sum(theta[this_action]*this_state))\n",
    "            #print()\n",
    "            #q[(this_state, this_action)] = q[(this_state, this_action)] + alpha*(reward+gamma*q[(next_state, next_action)] - q[(this_state, this_action)])\n",
    "\n",
    "            this_state = next_state\n",
    "            #this_state_hash = next_state_hash\n",
    "            this_action = next_action\n",
    "            this_q = next_q\n",
    "\n",
    "            this_episode_rewards.append(reward)\n",
    "            #counter += 1\n",
    "            #if finished:\n",
    "            #    env.render()\n",
    "            #if counter%10000 == 0:\n",
    "            #    print(theta.reshape((3,3,9,9))[:,0,:,:])\n",
    "            #    print()\n",
    "            #    print()\n",
    "            #if episode_id%200 == 0:\n",
    "            #    #print(episode_id)\n",
    "            #    env.render()\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "        this_agent_rewards.append(sum(this_episode_rewards))\n",
    "    sarsa_rewards.append(this_agent_rewards)\n",
    "\n",
    "#print(np.max(theta))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "#from matplotlib import pyplot as plt\n",
    "#print(theta[0][:121])\n",
    "print(np.sum([1 for x in sarsa_rewards[0] if x >-200 ]))\n",
    "print(np.sum([1 for x in sarsa_rewards[0] if x ==-200 ]))\n",
    "#print(np.sum(theta[2]*piler.get_vector([-0.1,-0.01])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "#epsilon = 0\n",
    "episode_id=0\n",
    "while episode_id < 100:\n",
    "    episode_id += 1\n",
    "    if episode_id%10==0:\n",
    "        print(episode_id)\n",
    "    this_episode_sap = []\n",
    "    this_episode_rewards = []\n",
    "    this_state = env.reset()\n",
    "    #print(this_state)\n",
    "    this_state = piler.get_vector(this_state)\n",
    "    this_state_hash = hash(str(this_state))\n",
    "    #print(this_state_hash)\n",
    "    terminal = False\n",
    "    pi, q = init_pi_q(pi, q, pi_init, q_init, this_state_hash, actions, terminal)\n",
    "    q_all_a = np.array([np.sum(theta[x]*this_state) for x in actions])\n",
    "    a_star = np.random.choice(np.flatnonzero(q_all_a == q_all_a.max()))\n",
    "    pi[this_state_hash] = [epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions]\n",
    "    this_action = np.random.choice(actions, p=pi[this_state_hash])\n",
    "    #print(theta[this_action].shape,this_state.shape)\n",
    "    this_q = np.sum(theta[this_action]*this_state)\n",
    "\n",
    "    #print(q_all_a)\n",
    "    #print(pi[this_state_hash])\n",
    "    #(next_state, reward, terminal, _) = env.step(this_action)\n",
    "    #print(next_state)\n",
    "    #print(reward)\n",
    "    #print(terminal)\n",
    "    counter = 0\n",
    "    while True:\n",
    "        (next_state, reward, terminal, _) = env.step(this_action)\n",
    "        #print(next_state)\n",
    "        next_state = piler.get_vector(next_state)\n",
    "        next_state_hash = hash(str(next_state))\n",
    "        pi, q = init_pi_q(pi, q, pi_init, q_init, next_state_hash, actions, terminal)\n",
    "        q_all_a = np.array([np.sum(theta[x]*next_state) for x in actions])\n",
    "        a_star = np.random.choice(np.flatnonzero(q_all_a == q_all_a.max()))\n",
    "        pi[next_state_hash] = [epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions]\n",
    "        next_action = np.random.choice(actions, p=pi[next_state_hash])\n",
    "        next_q = np.sum(theta[next_action]*next_state)\n",
    "        #print(next_action)\n",
    "        #print(np.sum(theta[this_action]*this_state))\n",
    "        #theta[this_action] = theta[this_action] +alpha * (reward+gamma*next_q-this_q)*this_state\n",
    "        #print(np.sum(theta[this_action]*this_state))\n",
    "        #print()\n",
    "        #q[(this_state, this_action)] = q[(this_state, this_action)] + alpha*(reward+gamma*q[(next_state, next_action)] - q[(this_state, this_action)])\n",
    "\n",
    "        this_state = next_state\n",
    "        this_state_hash = next_state_hash\n",
    "        this_action = next_action\n",
    "        this_q = next_q\n",
    "\n",
    "        this_episode_rewards.append(reward)\n",
    "        counter += 1\n",
    "        env.render()\n",
    "        #if episode_id%100 == 0:\n",
    "            #env.render()\n",
    "        if terminal:\n",
    "            #print(counter)\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.000013135092747\n",
      "-10.000016976805657\n",
      "-10.000001179422672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-5.2924824e-01,  1.4771317e-04], dtype=float32), -1.0, False, {})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sum(theta[0]*next_state))\n",
    "print(np.sum(theta[1]*next_state))\n",
    "print(np.sum(theta[2]*next_state))\n",
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_control(env, piler, alpha, gamma, epsilon, pi_init, q_init, num_episodes, num_agents):\n",
    "    actions = actions_shared\n",
    "    num_actions = len(actions)\n",
    "    \n",
    "    sarsa_rewards = []\n",
    "    agent_id = 0\n",
    "    \n",
    "    pi_init = [1/num_actions for _ in range(num_actions)]\n",
    "    q_init = 0\n",
    "\n",
    "    while agent_id < num_agents:\n",
    "        agent_id += 1\n",
    "        \n",
    "        # Initialize policy \"pi\", state-action-pair-values \"q\" and dict of returns\n",
    "        # Dictionaries will be filled during training, Epsilon-soft pi guarantees that each state will be visited (for infinite episodes)\n",
    "        pi = {}\n",
    "        sap = {}\n",
    "        q = {}\n",
    "        returns = {}\n",
    "        theta = np.zeros((len(actions), piler.amount, piler.dimension[0]+1, piler.dimension[1]+1))\n",
    "        \n",
    "        episode_id = 0\n",
    "        this_agent_rewards = []\n",
    "\n",
    "        while episode_id < num_episodes:\n",
    "            episode_id += 1\n",
    "            \n",
    "            this_episode_sap = []\n",
    "            this_episode_rewards = []\n",
    "            this_state = env.reset()\n",
    "            terminal = False\n",
    "            pi, q = init_pi_q(pi, q, pi_init, q_init, this_state, actions, terminal)\n",
    "            q_all_a = np.array([q[(this_state,x)] for x in actions])\n",
    "            a_star = np.random.choice(np.flatnonzero(q_all_a == q_all_a.max()))\n",
    "            pi[this_state] = [epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions]\n",
    "            this_action = np.random.choice(actions, p=pi[this_state])\n",
    "            while True:\n",
    "                next_state, reward, terminal = env.step(this_action)\n",
    "                pi, q = init_pi_q(pi, q, pi_init, q_init, next_state, actions, terminal)\n",
    "                q_all_a = np.array([q[(next_state,x)] for x in actions])\n",
    "                a_star = np.random.choice(np.flatnonzero(q_all_a == q_all_a.max()))\n",
    "                pi[next_state] = [epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions]\n",
    "                next_action = np.random.choice(actions, p=pi[next_state])\n",
    "                \n",
    "                q[(this_state, this_action)] = q[(this_state, this_action)] + alpha*(reward+gamma*q[(next_state, next_action)] - q[(this_state, this_action)])\n",
    "                \n",
    "                this_state = next_state\n",
    "                this_action = next_action\n",
    "                \n",
    "                this_episode_rewards.append(reward)\n",
    "                \n",
    "                if terminal:\n",
    "                    break\n",
    "            \n",
    "            this_agent_rewards.append(sum(this_episode_rewards))\n",
    "        sarsa_rewards.append(this_agent_rewards)\n",
    "    \n",
    "    return sarsa_rewards\n",
    "\n",
    "alpha = alpha_shared\n",
    "gamma = gamma_shared\n",
    "epsilon = epsilon_shared\n",
    "num_episodes = num_episodes_shared\n",
    "num_agents = num_agents_shared\n",
    "pi_init = pi_init_shared\n",
    "q_init = q_init_shared\n",
    "\n",
    "sarsa_rewards = sarsa_control(env, piler, alpha, gamma, epsilon, pi_init, q_init, num_episodes, num_agents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mountaincar]",
   "language": "python",
   "name": "conda-env-mountaincar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

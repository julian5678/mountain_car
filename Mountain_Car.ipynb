{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Pile_layer():\n",
    "    '''\n",
    "    Class to seperate a continuous environment into grids with the number of single grids for each dimension specified in \"dimension\"\n",
    "    The amount of (overlapping) grids can be specified in amount with the \"offset\", each grid will be moved by offset\n",
    "    '''\n",
    "    def __init__(self, dimension, input_dim, amount, offset):\n",
    "        self.dimension = dimension\n",
    "        self.amount = amount\n",
    "        self.offset = offset\n",
    "        self.input_dim = input_dim\n",
    "        self.initialize_layer()\n",
    "        self.total_dimension = (dimension[0]+1)*(dimension[1]+1)*amount\n",
    "    \n",
    "    def initialize_layer(self):\n",
    "        width = []\n",
    "        self.x_ranges = []\n",
    "        self.y_ranges = []\n",
    "        for i in range(len(self.dimension)):\n",
    "            width.append((input_dim[i][1]-input_dim[i][0])/(self.dimension[i]))\n",
    "        for num in range(self.amount):\n",
    "            this_x_offset = self.offset[0]*width[0]*num\n",
    "            this_y_offset = self.offset[1]*width[1]*num\n",
    "            this_x_ranges = []\n",
    "            this_y_ranges = []\n",
    "            #calculate grid limits for x direction\n",
    "            for i in range(self.dimension[0]+1):\n",
    "                if i == 0:\n",
    "                    this_x_ranges.append(self.input_dim[0][0])\n",
    "                    if this_x_offset != 0:\n",
    "                        this_x_ranges.append(self.input_dim[0][0]+this_x_offset+width[0]*i)\n",
    "                elif i == self.dimension[0]:\n",
    "                    this_x_ranges.append(self.input_dim[0][1])\n",
    "                    if this_x_offset == 0:\n",
    "                        this_x_ranges.append(-np.inf)\n",
    "                else:\n",
    "                    this_x_ranges.append(self.input_dim[0][0]+this_x_offset+width[0]*i)                        \n",
    "            \n",
    "            #calculate grid limits for y direction\n",
    "            for i in range(self.dimension[1]+1):\n",
    "                if i == 0:\n",
    "                    this_y_ranges.append(self.input_dim[1][0])\n",
    "                    if this_y_offset != 0:\n",
    "                        this_y_ranges.append(self.input_dim[1][0]+this_y_offset+width[1]*i)\n",
    "                elif i == self.dimension[1]:\n",
    "                    this_y_ranges.append(self.input_dim[1][1])\n",
    "                    if this_y_offset == 0:\n",
    "                        this_y_ranges.append(-np.inf)                    \n",
    "                else:\n",
    "                    this_y_ranges.append(self.input_dim[1][0]+this_y_offset+width[1]*i) \n",
    "        \n",
    "            self.x_ranges.append(this_x_ranges)\n",
    "            self.y_ranges.append(this_y_ranges)\n",
    "            \n",
    "        self.x_ranges = np.asarray(self.x_ranges)\n",
    "        self.y_ranges = np.asarray(self.y_ranges)\n",
    "    \n",
    "    def get_vector(self, state):\n",
    "        state_matrix = np.zeros((self.amount, self.dimension[0]+1, self.dimension[1]+1))\n",
    "        for num in range(self.amount):\n",
    "            x_tile = np.argmin([np.inf if x<=0 else x for x in self.x_ranges[num]-state[0]])-1\n",
    "            y_tile = np.argmin([np.inf if x<=0 else x for x in self.y_ranges[num]-state[1]])-1\n",
    "            state_matrix[num, x_tile, y_tile] = 1\n",
    "        \n",
    "        return state_matrix.flatten()\n",
    "            \n",
    "input_dim = [[env.observation_space.low[0],env.observation_space.high[0]],[env.observation_space.low[1],env.observation_space.high[1]]]\n",
    "piler = Pile_layer([15,15],input_dim, 5, [0.2,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.1 #Learning rate\n",
    "gamma = 0.95 #Discount factor\n",
    "epsilon_start = 0.15 #epsilon at the start of training\n",
    "num_episodes = 1000 \n",
    "\n",
    "actions = [0,1,2]\n",
    "theta = np.full((3, piler.total_dimension), 1, dtype=float) #weights\n",
    "\n",
    "num_actions=len(actions)\n",
    "episode_id = 0\n",
    "rewards = []\n",
    "\n",
    "while episode_id < num_episodes:\n",
    "    # linearly reduce epsilon\n",
    "    episode_id += 1\n",
    "    epsilon = epsilon_start-epsilon_start/(num_episodes)*episode_id\n",
    "\n",
    "    this_episode_rewards = []\n",
    "    this_state = env.reset()\n",
    "    this_state = piler.get_vector(this_state)\n",
    "    terminal = False\n",
    "    \n",
    "    #Update policy and randomly choose next action\n",
    "    q_all_a = np.array([np.sum(theta[x]*this_state) for x in actions])\n",
    "    a_star = np.random.choice(np.flatnonzero(q_all_a == q_all_a.max()))\n",
    "    this_action = np.random.choice(actions, p=[epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions])\n",
    "    this_q = np.sum(theta[this_action]*this_state)\n",
    "    while True:\n",
    "        (next_state, reward, terminal, _) = env.step(this_action)\n",
    "        \n",
    "        #Update policy and randomly choose next state\n",
    "        next_state = piler.get_vector(next_state)\n",
    "        q_all_a = np.array([np.sum(theta[x]*next_state) for x in actions])\n",
    "        a_star = np.random.choice(np.flatnonzero(q_all_a == q_all_a.max()))\n",
    "        next_action = np.random.choice(actions, p=[epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions])\n",
    "        \n",
    "        #Update weights\n",
    "        next_q = np.sum(theta[next_action]*next_state)\n",
    "        theta[this_action] = theta[this_action] +alpha * (reward+gamma*next_q-this_q)*this_state\n",
    "        \n",
    "        this_state = next_state\n",
    "        this_action = next_action\n",
    "        this_q = next_q\n",
    "        this_episode_rewards.append(reward)\n",
    "        \n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    rewards.append(sum(this_episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the result of the training\n",
    "epsilon = 0\n",
    "num_episodes = 10\n",
    "\n",
    "episode_id=0\n",
    "while episode_id < num_episodes:\n",
    "    episode_id += 1\n",
    "    \n",
    "    this_state = env.reset()\n",
    "    this_state = piler.get_vector(this_state)\n",
    "    terminal = False\n",
    "    \n",
    "    q_all_a = np.array([np.sum(theta[x]*this_state) for x in actions])\n",
    "    a_star = np.random.choice(np.flatnonzero(q_all_a == q_all_a.max()))\n",
    "    this_action = np.random.choice(actions, p=[epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions])\n",
    "    while True:\n",
    "        (next_state, reward, terminal, _) = env.step(this_action)\n",
    "        next_state = piler.get_vector(next_state)\n",
    "        \n",
    "        # update policy to select next action\n",
    "        q_all_a = np.array([np.sum(theta[x]*next_state) for x in actions])\n",
    "        a_star = np.random.choice(np.flatnonzero(q_all_a == q_all_a.max()))\n",
    "        next_action = np.random.choice(actions, p=[epsilon/num_actions if x!=a_star else 1-epsilon+epsilon/num_actions for x in actions])\n",
    "\n",
    "        this_state = next_state\n",
    "        this_action = next_action\n",
    "        env.render()\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mountaincar]",
   "language": "python",
   "name": "conda-env-mountaincar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
